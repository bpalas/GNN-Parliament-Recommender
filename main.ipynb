{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "enriquecido_df = pd.read_csv('gnn_recommender.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional \n",
    "enriquecido_df = enriquecido_df.sample(frac=0.7, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Extraer la información de nodos (parlamentarios)\n",
    "# Nos quedamos con las columnas relevantes para los parlamentarios (person_1 y person_2 con sus descripciones)\n",
    "nodos_1 = enriquecido_df[['parliamentarian_1', 'biografia_1', 'region_1', 'partido_1', 'sector_1']]\n",
    "nodos_2 = enriquecido_df[['parliamentarian_2', 'biografia_2', 'region_2', 'partido_2', 'sector_2']]\n",
    "\n",
    "# Renombrar las columnas de nodos_2 para que coincidan con nodos_1\n",
    "nodos_2.columns = ['parliamentarian_1', 'biografia_1', 'region_1', 'partido_1', 'sector_1']\n",
    "\n",
    "# Concatenar los nodos de ambos parlamentarios (quitar duplicados)\n",
    "nodos = pd.concat([nodos_1, nodos_2], axis=0).drop_duplicates(subset='parliamentarian_1').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 2. Extraer la información de aristas (interacciones)\n",
    "# Nos quedamos con las columnas relevantes para las interacciones entre parlamentarios\n",
    "aristas = enriquecido_df[['parliamentarian_1', 'parliamentarian_2', 'proportion_agreement', 'region_1', 'region_2', 'partido_1', 'partido_2', 'sector_1', 'sector_2']]\n",
    "aristas = aristas.drop_duplicates(subset=['parliamentarian_1', 'parliamentarian_2']).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\benja\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c082c1ba374709b7788478a35e5055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benja\\AppData\\Local\\Temp\\ipykernel_35348\\1858818977.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  edge_index = torch.tensor([aristas['source'].values, aristas['target'].values], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9075, Train: 1.1724, Val: 1.1746\n",
      "Epoch: 002, Loss: 1.3746, Train: 0.7189, Val: 0.7419\n",
      "Epoch: 003, Loss: 0.5169, Train: 0.6281, Val: 0.5814\n",
      "Epoch: 004, Loss: 0.3945, Train: 0.5214, Val: 0.4871\n",
      "Epoch: 005, Loss: 0.2719, Train: 0.3343, Val: 0.3587\n",
      "Epoch: 006, Loss: 0.1117, Train: 0.4768, Val: 0.5049\n",
      "Epoch: 007, Loss: 0.2273, Train: 0.4923, Val: 0.5185\n",
      "Epoch: 008, Loss: 0.2423, Train: 0.3809, Val: 0.4076\n",
      "Epoch: 009, Loss: 0.1451, Train: 0.3482, Val: 0.3465\n",
      "Epoch: 010, Loss: 0.1212, Train: 0.4518, Val: 0.4256\n",
      "Epoch: 011, Loss: 0.2041, Train: 0.3749, Val: 0.3637\n",
      "Epoch: 012, Loss: 0.1405, Train: 0.3265, Val: 0.3435\n",
      "Epoch: 013, Loss: 0.1066, Train: 0.3743, Val: 0.3991\n",
      "Epoch: 014, Loss: 0.1401, Train: 0.3949, Val: 0.4202\n",
      "Epoch: 015, Loss: 0.1559, Train: 0.3674, Val: 0.3916\n",
      "Epoch: 016, Loss: 0.1350, Train: 0.3277, Val: 0.3449\n",
      "Epoch: 017, Loss: 0.1074, Train: 0.3345, Val: 0.3350\n",
      "Epoch: 018, Loss: 0.1119, Train: 0.3674, Val: 0.3559\n",
      "Epoch: 019, Loss: 0.1350, Train: 0.3570, Val: 0.3484\n",
      "Epoch: 020, Loss: 0.1275, Train: 0.3262, Val: 0.3313\n",
      "Epoch: 021, Loss: 0.1064, Train: 0.3280, Val: 0.3450\n",
      "Epoch: 022, Loss: 0.1076, Train: 0.3472, Val: 0.3688\n",
      "Epoch: 023, Loss: 0.1205, Train: 0.3504, Val: 0.3723\n",
      "Epoch: 024, Loss: 0.1228, Train: 0.3344, Val: 0.3535\n",
      "Epoch: 025, Loss: 0.1118, Train: 0.3217, Val: 0.3331\n",
      "Epoch: 026, Loss: 0.1035, Train: 0.3303, Val: 0.3313\n",
      "Epoch: 027, Loss: 0.1091, Train: 0.3410, Val: 0.3368\n",
      "Epoch: 028, Loss: 0.1163, Train: 0.3327, Val: 0.3321\n",
      "Epoch: 029, Loss: 0.1107, Train: 0.3215, Val: 0.3296\n",
      "Epoch: 030, Loss: 0.1034, Train: 0.3250, Val: 0.3402\n",
      "Epoch: 031, Loss: 0.1056, Train: 0.3327, Val: 0.3508\n",
      "Epoch: 032, Loss: 0.1107, Train: 0.3312, Val: 0.3487\n",
      "Epoch: 033, Loss: 0.1097, Train: 0.3231, Val: 0.3370\n",
      "Epoch: 034, Loss: 0.1044, Train: 0.3209, Val: 0.3284\n",
      "Epoch: 035, Loss: 0.1030, Train: 0.3266, Val: 0.3284\n",
      "Epoch: 036, Loss: 0.1067, Train: 0.3280, Val: 0.3288\n",
      "Epoch: 037, Loss: 0.1076, Train: 0.3223, Val: 0.3272\n",
      "Epoch: 038, Loss: 0.1039, Train: 0.3200, Val: 0.3304\n",
      "Epoch: 039, Loss: 0.1024, Train: 0.3234, Val: 0.3375\n",
      "Epoch: 040, Loss: 0.1046, Train: 0.3250, Val: 0.3398\n",
      "Epoch: 041, Loss: 0.1056, Train: 0.3218, Val: 0.3348\n",
      "Epoch: 042, Loss: 0.1036, Train: 0.3193, Val: 0.3284\n",
      "Epoch: 043, Loss: 0.1019, Train: 0.3211, Val: 0.3260\n",
      "Epoch: 044, Loss: 0.1031, Train: 0.3227, Val: 0.3260\n",
      "Epoch: 045, Loss: 0.1042, Train: 0.3206, Val: 0.3256\n",
      "Epoch: 046, Loss: 0.1028, Train: 0.3188, Val: 0.3274\n",
      "Epoch: 047, Loss: 0.1016, Train: 0.3200, Val: 0.3314\n",
      "Epoch: 048, Loss: 0.1024, Train: 0.3210, Val: 0.3333\n",
      "Epoch: 049, Loss: 0.1030, Train: 0.3196, Val: 0.3308\n",
      "Epoch: 050, Loss: 0.1022, Train: 0.3183, Val: 0.3268\n",
      "Epoch: 051, Loss: 0.1013, Train: 0.3190, Val: 0.3248\n",
      "Epoch: 052, Loss: 0.1018, Train: 0.3197, Val: 0.3244\n",
      "Epoch: 053, Loss: 0.1022, Train: 0.3186, Val: 0.3245\n",
      "Epoch: 054, Loss: 0.1015, Train: 0.3178, Val: 0.3261\n",
      "Epoch: 055, Loss: 0.1010, Train: 0.3184, Val: 0.3285\n",
      "Epoch: 056, Loss: 0.1014, Train: 0.3187, Val: 0.3291\n",
      "Epoch: 057, Loss: 0.1016, Train: 0.3178, Val: 0.3272\n",
      "Epoch: 058, Loss: 0.1010, Train: 0.3174, Val: 0.3248\n",
      "Epoch: 059, Loss: 0.1007, Train: 0.3178, Val: 0.3236\n",
      "Epoch: 060, Loss: 0.1010, Train: 0.3178, Val: 0.3234\n",
      "Epoch: 061, Loss: 0.1010, Train: 0.3171, Val: 0.3239\n",
      "Epoch: 062, Loss: 0.1006, Train: 0.3170, Val: 0.3253\n",
      "Epoch: 063, Loss: 0.1005, Train: 0.3173, Val: 0.3265\n",
      "Epoch: 064, Loss: 0.1007, Train: 0.3171, Val: 0.3260\n",
      "Epoch: 065, Loss: 0.1006, Train: 0.3166, Val: 0.3244\n",
      "Epoch: 066, Loss: 0.1003, Train: 0.3167, Val: 0.3231\n",
      "Epoch: 067, Loss: 0.1003, Train: 0.3168, Val: 0.3226\n",
      "Epoch: 068, Loss: 0.1004, Train: 0.3165, Val: 0.3228\n",
      "Epoch: 069, Loss: 0.1002, Train: 0.3162, Val: 0.3235\n",
      "Epoch: 070, Loss: 0.1000, Train: 0.3163, Val: 0.3244\n",
      "Epoch: 071, Loss: 0.1001, Train: 0.3163, Val: 0.3244\n",
      "Epoch: 072, Loss: 0.1000, Train: 0.3160, Val: 0.3235\n",
      "Epoch: 073, Loss: 0.0999, Train: 0.3159, Val: 0.3225\n",
      "Epoch: 074, Loss: 0.0998, Train: 0.3160, Val: 0.3220\n",
      "Epoch: 075, Loss: 0.0998, Train: 0.3158, Val: 0.3220\n",
      "Epoch: 076, Loss: 0.0997, Train: 0.3156, Val: 0.3225\n",
      "Epoch: 077, Loss: 0.0996, Train: 0.3156, Val: 0.3231\n",
      "Epoch: 078, Loss: 0.0996, Train: 0.3156, Val: 0.3231\n",
      "Epoch: 079, Loss: 0.0996, Train: 0.3154, Val: 0.3225\n",
      "Epoch: 080, Loss: 0.0995, Train: 0.3153, Val: 0.3217\n",
      "Epoch: 081, Loss: 0.0994, Train: 0.3153, Val: 0.3214\n",
      "Epoch: 082, Loss: 0.0994, Train: 0.3152, Val: 0.3214\n",
      "Epoch: 083, Loss: 0.0994, Train: 0.3151, Val: 0.3217\n",
      "Epoch: 084, Loss: 0.0993, Train: 0.3151, Val: 0.3221\n",
      "Epoch: 085, Loss: 0.0993, Train: 0.3150, Val: 0.3220\n",
      "Epoch: 086, Loss: 0.0992, Train: 0.3149, Val: 0.3215\n",
      "Epoch: 087, Loss: 0.0992, Train: 0.3149, Val: 0.3210\n",
      "Epoch: 088, Loss: 0.0991, Train: 0.3148, Val: 0.3208\n",
      "Epoch: 089, Loss: 0.0991, Train: 0.3147, Val: 0.3209\n",
      "Epoch: 090, Loss: 0.0990, Train: 0.3146, Val: 0.3211\n",
      "Epoch: 091, Loss: 0.0990, Train: 0.3146, Val: 0.3213\n",
      "Epoch: 092, Loss: 0.0990, Train: 0.3145, Val: 0.3211\n",
      "Epoch: 093, Loss: 0.0989, Train: 0.3145, Val: 0.3207\n",
      "Epoch: 094, Loss: 0.0989, Train: 0.3144, Val: 0.3204\n",
      "Epoch: 095, Loss: 0.0989, Train: 0.3144, Val: 0.3203\n",
      "Epoch: 096, Loss: 0.0988, Train: 0.3143, Val: 0.3204\n",
      "Epoch: 097, Loss: 0.0988, Train: 0.3142, Val: 0.3206\n",
      "Epoch: 098, Loss: 0.0987, Train: 0.3142, Val: 0.3206\n",
      "Epoch: 099, Loss: 0.0987, Train: 0.3141, Val: 0.3204\n",
      "Epoch: 100, Loss: 0.0987, Train: 0.3141, Val: 0.3201\n",
      "Epoch: 101, Loss: 0.0986, Train: 0.3140, Val: 0.3199\n",
      "Epoch: 102, Loss: 0.0986, Train: 0.3140, Val: 0.3199\n",
      "Epoch: 103, Loss: 0.0986, Train: 0.3139, Val: 0.3200\n",
      "Epoch: 104, Loss: 0.0985, Train: 0.3139, Val: 0.3201\n",
      "Epoch: 105, Loss: 0.0985, Train: 0.3138, Val: 0.3199\n",
      "Epoch: 106, Loss: 0.0985, Train: 0.3138, Val: 0.3197\n",
      "Epoch: 107, Loss: 0.0984, Train: 0.3137, Val: 0.3196\n",
      "Epoch: 108, Loss: 0.0984, Train: 0.3137, Val: 0.3195\n",
      "Epoch: 109, Loss: 0.0984, Train: 0.3136, Val: 0.3196\n",
      "Epoch: 110, Loss: 0.0983, Train: 0.3136, Val: 0.3196\n",
      "Epoch: 111, Loss: 0.0983, Train: 0.3135, Val: 0.3195\n",
      "Epoch: 112, Loss: 0.0983, Train: 0.3135, Val: 0.3194\n",
      "Epoch: 113, Loss: 0.0983, Train: 0.3134, Val: 0.3192\n",
      "Epoch: 114, Loss: 0.0982, Train: 0.3134, Val: 0.3192\n",
      "Epoch: 115, Loss: 0.0982, Train: 0.3133, Val: 0.3192\n",
      "Epoch: 116, Loss: 0.0982, Train: 0.3133, Val: 0.3192\n",
      "Epoch: 117, Loss: 0.0982, Train: 0.3132, Val: 0.3192\n",
      "Epoch: 118, Loss: 0.0981, Train: 0.3132, Val: 0.3190\n",
      "Epoch: 119, Loss: 0.0981, Train: 0.3132, Val: 0.3189\n",
      "Epoch: 120, Loss: 0.0981, Train: 0.3131, Val: 0.3189\n",
      "Epoch: 121, Loss: 0.0980, Train: 0.3131, Val: 0.3189\n",
      "Epoch: 122, Loss: 0.0980, Train: 0.3130, Val: 0.3189\n",
      "Epoch: 123, Loss: 0.0980, Train: 0.3130, Val: 0.3188\n",
      "Epoch: 124, Loss: 0.0980, Train: 0.3130, Val: 0.3188\n",
      "Epoch: 125, Loss: 0.0979, Train: 0.3129, Val: 0.3187\n",
      "Epoch: 126, Loss: 0.0979, Train: 0.3129, Val: 0.3186\n",
      "Epoch: 127, Loss: 0.0979, Train: 0.3128, Val: 0.3186\n",
      "Epoch: 128, Loss: 0.0979, Train: 0.3128, Val: 0.3186\n",
      "Epoch: 129, Loss: 0.0979, Train: 0.3128, Val: 0.3186\n",
      "Epoch: 130, Loss: 0.0978, Train: 0.3127, Val: 0.3185\n",
      "Epoch: 131, Loss: 0.0978, Train: 0.3127, Val: 0.3184\n",
      "Epoch: 132, Loss: 0.0978, Train: 0.3127, Val: 0.3184\n",
      "Epoch: 133, Loss: 0.0978, Train: 0.3126, Val: 0.3184\n",
      "Epoch: 134, Loss: 0.0977, Train: 0.3126, Val: 0.3184\n",
      "Epoch: 135, Loss: 0.0977, Train: 0.3126, Val: 0.3183\n",
      "Epoch: 136, Loss: 0.0977, Train: 0.3125, Val: 0.3183\n",
      "Epoch: 137, Loss: 0.0977, Train: 0.3125, Val: 0.3182\n",
      "Epoch: 138, Loss: 0.0977, Train: 0.3125, Val: 0.3182\n",
      "Epoch: 139, Loss: 0.0976, Train: 0.3124, Val: 0.3182\n",
      "Epoch: 140, Loss: 0.0976, Train: 0.3124, Val: 0.3182\n",
      "Epoch: 141, Loss: 0.0976, Train: 0.3124, Val: 0.3181\n",
      "Epoch: 142, Loss: 0.0976, Train: 0.3123, Val: 0.3181\n",
      "Epoch: 143, Loss: 0.0976, Train: 0.3123, Val: 0.3181\n",
      "Epoch: 144, Loss: 0.0975, Train: 0.3123, Val: 0.3181\n",
      "Epoch: 145, Loss: 0.0975, Train: 0.3123, Val: 0.3180\n",
      "Epoch: 146, Loss: 0.0975, Train: 0.3122, Val: 0.3180\n",
      "Epoch: 147, Loss: 0.0975, Train: 0.3122, Val: 0.3180\n",
      "Epoch: 148, Loss: 0.0975, Train: 0.3122, Val: 0.3179\n",
      "Epoch: 149, Loss: 0.0975, Train: 0.3121, Val: 0.3179\n",
      "Epoch: 150, Loss: 0.0974, Train: 0.3121, Val: 0.3179\n",
      "Epoch: 151, Loss: 0.0974, Train: 0.3121, Val: 0.3179\n",
      "Epoch: 152, Loss: 0.0974, Train: 0.3121, Val: 0.3179\n",
      "Epoch: 153, Loss: 0.0974, Train: 0.3120, Val: 0.3179\n",
      "Epoch: 154, Loss: 0.0974, Train: 0.3120, Val: 0.3178\n",
      "Epoch: 155, Loss: 0.0973, Train: 0.3120, Val: 0.3178\n",
      "Epoch: 156, Loss: 0.0973, Train: 0.3120, Val: 0.3178\n",
      "Epoch: 157, Loss: 0.0973, Train: 0.3119, Val: 0.3178\n",
      "Epoch: 158, Loss: 0.0973, Train: 0.3119, Val: 0.3178\n",
      "Epoch: 159, Loss: 0.0973, Train: 0.3119, Val: 0.3178\n",
      "Epoch: 160, Loss: 0.0973, Train: 0.3119, Val: 0.3178\n",
      "Epoch: 161, Loss: 0.0973, Train: 0.3118, Val: 0.3178\n",
      "Epoch: 162, Loss: 0.0972, Train: 0.3118, Val: 0.3178\n",
      "Epoch: 163, Loss: 0.0972, Train: 0.3118, Val: 0.3177\n",
      "Epoch: 164, Loss: 0.0972, Train: 0.3118, Val: 0.3177\n",
      "Epoch: 165, Loss: 0.0972, Train: 0.3117, Val: 0.3177\n",
      "Epoch: 166, Loss: 0.0972, Train: 0.3117, Val: 0.3177\n",
      "Epoch: 167, Loss: 0.0972, Train: 0.3117, Val: 0.3177\n",
      "Epoch: 168, Loss: 0.0972, Train: 0.3117, Val: 0.3177\n",
      "Epoch: 169, Loss: 0.0971, Train: 0.3116, Val: 0.3177\n",
      "Epoch: 170, Loss: 0.0971, Train: 0.3116, Val: 0.3177\n",
      "Epoch: 171, Loss: 0.0971, Train: 0.3116, Val: 0.3177\n",
      "Epoch: 172, Loss: 0.0971, Train: 0.3116, Val: 0.3177\n",
      "Epoch: 173, Loss: 0.0971, Train: 0.3116, Val: 0.3177\n",
      "Epoch: 174, Loss: 0.0971, Train: 0.3115, Val: 0.3177\n",
      "Epoch: 175, Loss: 0.0971, Train: 0.3115, Val: 0.3177\n",
      "Epoch: 176, Loss: 0.0970, Train: 0.3115, Val: 0.3177\n",
      "Epoch: 177, Loss: 0.0970, Train: 0.3115, Val: 0.3176\n",
      "Epoch: 178, Loss: 0.0970, Train: 0.3115, Val: 0.3176\n",
      "Epoch: 179, Loss: 0.0970, Train: 0.3114, Val: 0.3176\n",
      "Epoch: 180, Loss: 0.0970, Train: 0.3114, Val: 0.3176\n",
      "Epoch: 181, Loss: 0.0970, Train: 0.3114, Val: 0.3176\n",
      "Epoch: 182, Loss: 0.0970, Train: 0.3114, Val: 0.3176\n",
      "Epoch: 183, Loss: 0.0970, Train: 0.3114, Val: 0.3176\n",
      "Epoch: 184, Loss: 0.0969, Train: 0.3113, Val: 0.3176\n",
      "Epoch: 185, Loss: 0.0969, Train: 0.3113, Val: 0.3176\n",
      "Epoch: 186, Loss: 0.0969, Train: 0.3113, Val: 0.3176\n",
      "Epoch: 187, Loss: 0.0969, Train: 0.3113, Val: 0.3176\n",
      "Epoch: 188, Loss: 0.0969, Train: 0.3113, Val: 0.3176\n",
      "Epoch: 189, Loss: 0.0969, Train: 0.3112, Val: 0.3176\n",
      "Epoch: 190, Loss: 0.0969, Train: 0.3112, Val: 0.3176\n",
      "Epoch: 191, Loss: 0.0969, Train: 0.3112, Val: 0.3176\n",
      "Epoch: 192, Loss: 0.0969, Train: 0.3112, Val: 0.3176\n",
      "Epoch: 193, Loss: 0.0968, Train: 0.3112, Val: 0.3176\n",
      "Epoch: 194, Loss: 0.0968, Train: 0.3112, Val: 0.3176\n",
      "Epoch: 195, Loss: 0.0968, Train: 0.3111, Val: 0.3176\n",
      "Epoch: 196, Loss: 0.0968, Train: 0.3111, Val: 0.3176\n",
      "Epoch: 197, Loss: 0.0968, Train: 0.3111, Val: 0.3176\n",
      "Epoch: 198, Loss: 0.0968, Train: 0.3111, Val: 0.3176\n",
      "Epoch: 199, Loss: 0.0968, Train: 0.3111, Val: 0.3176\n",
      "Epoch: 200, Loss: 0.0968, Train: 0.3111, Val: 0.3176\n",
      "Epoch: 201, Loss: 0.0968, Train: 0.3110, Val: 0.3176\n",
      "Epoch: 202, Loss: 0.0967, Train: 0.3110, Val: 0.3176\n",
      "Epoch: 203, Loss: 0.0967, Train: 0.3110, Val: 0.3176\n",
      "Epoch: 204, Loss: 0.0967, Train: 0.3110, Val: 0.3176\n",
      "Epoch: 205, Loss: 0.0967, Train: 0.3110, Val: 0.3176\n",
      "Epoch: 206, Loss: 0.0967, Train: 0.3110, Val: 0.3176\n",
      "Epoch: 207, Loss: 0.0967, Train: 0.3109, Val: 0.3176\n",
      "Epoch: 208, Loss: 0.0967, Train: 0.3109, Val: 0.3176\n",
      "Epoch: 209, Loss: 0.0967, Train: 0.3109, Val: 0.3176\n",
      "Epoch: 210, Loss: 0.0967, Train: 0.3109, Val: 0.3176\n",
      "Epoch: 211, Loss: 0.0967, Train: 0.3109, Val: 0.3176\n",
      "Epoch: 212, Loss: 0.0966, Train: 0.3109, Val: 0.3176\n",
      "Epoch: 213, Loss: 0.0966, Train: 0.3108, Val: 0.3176\n",
      "Epoch: 214, Loss: 0.0966, Train: 0.3108, Val: 0.3176\n",
      "Epoch: 215, Loss: 0.0966, Train: 0.3108, Val: 0.3176\n",
      "Epoch: 216, Loss: 0.0966, Train: 0.3108, Val: 0.3176\n",
      "Epoch: 217, Loss: 0.0966, Train: 0.3108, Val: 0.3176\n",
      "Epoch: 218, Loss: 0.0966, Train: 0.3108, Val: 0.3176\n",
      "Epoch: 219, Loss: 0.0966, Train: 0.3108, Val: 0.3176\n",
      "Epoch: 220, Loss: 0.0966, Train: 0.3107, Val: 0.3176\n",
      "Epoch: 221, Loss: 0.0966, Train: 0.3107, Val: 0.3176\n",
      "Epoch: 222, Loss: 0.0965, Train: 0.3107, Val: 0.3176\n",
      "Epoch: 223, Loss: 0.0965, Train: 0.3107, Val: 0.3176\n",
      "Epoch: 224, Loss: 0.0965, Train: 0.3107, Val: 0.3176\n",
      "Epoch: 225, Loss: 0.0965, Train: 0.3107, Val: 0.3176\n",
      "Epoch: 226, Loss: 0.0965, Train: 0.3106, Val: 0.3176\n",
      "Epoch: 227, Loss: 0.0965, Train: 0.3106, Val: 0.3176\n",
      "Epoch: 228, Loss: 0.0965, Train: 0.3106, Val: 0.3177\n",
      "Epoch: 229, Loss: 0.0965, Train: 0.3106, Val: 0.3177\n",
      "Epoch: 230, Loss: 0.0965, Train: 0.3106, Val: 0.3177\n",
      "Epoch: 231, Loss: 0.0965, Train: 0.3106, Val: 0.3177\n",
      "Epoch: 232, Loss: 0.0965, Train: 0.3106, Val: 0.3177\n",
      "Epoch: 233, Loss: 0.0964, Train: 0.3105, Val: 0.3177\n",
      "Epoch: 234, Loss: 0.0964, Train: 0.3105, Val: 0.3177\n",
      "Epoch: 235, Loss: 0.0964, Train: 0.3105, Val: 0.3177\n",
      "Epoch: 236, Loss: 0.0964, Train: 0.3105, Val: 0.3177\n",
      "Epoch: 237, Loss: 0.0964, Train: 0.3105, Val: 0.3177\n",
      "Epoch: 238, Loss: 0.0964, Train: 0.3105, Val: 0.3177\n",
      "Epoch: 239, Loss: 0.0964, Train: 0.3105, Val: 0.3177\n",
      "Epoch: 240, Loss: 0.0964, Train: 0.3104, Val: 0.3177\n",
      "Epoch: 241, Loss: 0.0964, Train: 0.3104, Val: 0.3177\n",
      "Epoch: 242, Loss: 0.0964, Train: 0.3104, Val: 0.3177\n",
      "Epoch: 243, Loss: 0.0964, Train: 0.3104, Val: 0.3177\n",
      "Epoch: 244, Loss: 0.0963, Train: 0.3104, Val: 0.3177\n",
      "Epoch: 245, Loss: 0.0963, Train: 0.3104, Val: 0.3177\n",
      "Epoch: 246, Loss: 0.0963, Train: 0.3104, Val: 0.3177\n",
      "Epoch: 247, Loss: 0.0963, Train: 0.3103, Val: 0.3177\n",
      "Epoch: 248, Loss: 0.0963, Train: 0.3103, Val: 0.3177\n",
      "Epoch: 249, Loss: 0.0963, Train: 0.3103, Val: 0.3177\n",
      "Epoch: 250, Loss: 0.0963, Train: 0.3103, Val: 0.3177\n",
      "Epoch: 251, Loss: 0.0963, Train: 0.3103, Val: 0.3177\n",
      "Epoch: 252, Loss: 0.0963, Train: 0.3103, Val: 0.3177\n",
      "Epoch: 253, Loss: 0.0963, Train: 0.3103, Val: 0.3177\n",
      "Epoch: 254, Loss: 0.0963, Train: 0.3103, Val: 0.3177\n",
      "Epoch: 255, Loss: 0.0963, Train: 0.3102, Val: 0.3177\n",
      "Epoch: 256, Loss: 0.0963, Train: 0.3102, Val: 0.3177\n",
      "Epoch: 257, Loss: 0.0962, Train: 0.3102, Val: 0.3177\n",
      "Epoch: 258, Loss: 0.0962, Train: 0.3102, Val: 0.3177\n",
      "Epoch: 259, Loss: 0.0962, Train: 0.3102, Val: 0.3177\n",
      "Epoch: 260, Loss: 0.0962, Train: 0.3102, Val: 0.3177\n",
      "Epoch: 261, Loss: 0.0962, Train: 0.3102, Val: 0.3177\n",
      "Epoch: 262, Loss: 0.0962, Train: 0.3102, Val: 0.3177\n",
      "Epoch: 263, Loss: 0.0962, Train: 0.3101, Val: 0.3177\n",
      "Epoch: 264, Loss: 0.0962, Train: 0.3101, Val: 0.3177\n",
      "Epoch: 265, Loss: 0.0962, Train: 0.3101, Val: 0.3177\n",
      "Epoch: 266, Loss: 0.0962, Train: 0.3101, Val: 0.3178\n",
      "Epoch: 267, Loss: 0.0962, Train: 0.3101, Val: 0.3178\n",
      "Epoch: 268, Loss: 0.0962, Train: 0.3101, Val: 0.3178\n",
      "Epoch: 269, Loss: 0.0961, Train: 0.3101, Val: 0.3178\n",
      "Epoch: 270, Loss: 0.0961, Train: 0.3101, Val: 0.3178\n",
      "Epoch: 271, Loss: 0.0961, Train: 0.3100, Val: 0.3178\n",
      "Epoch: 272, Loss: 0.0961, Train: 0.3100, Val: 0.3178\n",
      "Epoch: 273, Loss: 0.0961, Train: 0.3100, Val: 0.3178\n",
      "Epoch: 274, Loss: 0.0961, Train: 0.3100, Val: 0.3178\n",
      "Epoch: 275, Loss: 0.0961, Train: 0.3100, Val: 0.3178\n",
      "Epoch: 276, Loss: 0.0961, Train: 0.3100, Val: 0.3178\n",
      "Epoch: 277, Loss: 0.0961, Train: 0.3100, Val: 0.3178\n",
      "Epoch: 278, Loss: 0.0961, Train: 0.3100, Val: 0.3178\n",
      "Epoch: 279, Loss: 0.0961, Train: 0.3099, Val: 0.3178\n",
      "Epoch: 280, Loss: 0.0961, Train: 0.3099, Val: 0.3178\n",
      "Epoch: 281, Loss: 0.0961, Train: 0.3099, Val: 0.3178\n",
      "Epoch: 282, Loss: 0.0961, Train: 0.3099, Val: 0.3178\n",
      "Epoch: 283, Loss: 0.0960, Train: 0.3099, Val: 0.3178\n",
      "Epoch: 284, Loss: 0.0960, Train: 0.3099, Val: 0.3178\n",
      "Epoch: 285, Loss: 0.0960, Train: 0.3099, Val: 0.3178\n",
      "Epoch: 286, Loss: 0.0960, Train: 0.3099, Val: 0.3178\n",
      "Epoch: 287, Loss: 0.0960, Train: 0.3099, Val: 0.3179\n",
      "Epoch: 288, Loss: 0.0960, Train: 0.3098, Val: 0.3179\n",
      "Epoch: 289, Loss: 0.0960, Train: 0.3098, Val: 0.3179\n",
      "Epoch: 290, Loss: 0.0960, Train: 0.3098, Val: 0.3179\n",
      "Epoch: 291, Loss: 0.0960, Train: 0.3098, Val: 0.3179\n",
      "Epoch: 292, Loss: 0.0960, Train: 0.3098, Val: 0.3179\n",
      "Epoch: 293, Loss: 0.0960, Train: 0.3098, Val: 0.3179\n",
      "Epoch: 294, Loss: 0.0960, Train: 0.3098, Val: 0.3179\n",
      "Epoch: 295, Loss: 0.0960, Train: 0.3098, Val: 0.3179\n",
      "Epoch: 296, Loss: 0.0960, Train: 0.3098, Val: 0.3179\n",
      "Epoch: 297, Loss: 0.0959, Train: 0.3097, Val: 0.3179\n",
      "Epoch: 298, Loss: 0.0959, Train: 0.3097, Val: 0.3179\n",
      "Epoch: 299, Loss: 0.0959, Train: 0.3097, Val: 0.3179\n",
      "Epoch: 300, Loss: 0.0959, Train: 0.3097, Val: 0.3179\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paso 1: One-Hot Encoding de las columnas categóricas 'region_1', 'partido_1', 'sector_1'\n",
    "one_hot_features = pd.get_dummies(nodos[['region_1', 'partido_1', 'sector_1']])\n",
    "one_hot_tensor = torch.tensor(one_hot_features.values, dtype=torch.float)\n",
    "\n",
    "# Paso 2: Generar embeddings para las biografías usando SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "with torch.no_grad():\n",
    "    bio_embeddings = model.encode(nodos['biografia_1'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "    bio_embeddings = bio_embeddings.cpu()\n",
    "\n",
    "# Paso 3: Concatenar las características (One-Hot + Embeddings)\n",
    "node_features = torch.cat([one_hot_tensor, bio_embeddings], dim=-1)\n",
    "\n",
    "# Crear mapeo de IDs para los parlamentarios\n",
    "unique_parliamentarians = pd.concat([aristas['parliamentarian_1'], aristas['parliamentarian_2']]).unique()\n",
    "parliamentarian_to_index = {name: idx for idx, name in enumerate(unique_parliamentarians)}\n",
    "\n",
    "# Mapear los IDs en las aristas\n",
    "aristas['source'] = aristas['parliamentarian_1'].map(parliamentarian_to_index)\n",
    "aristas['target'] = aristas['parliamentarian_2'].map(parliamentarian_to_index)\n",
    "\n",
    "# Crear el edge_index para el grafo\n",
    "edge_index = torch.tensor([aristas['source'].values, aristas['target'].values], dtype=torch.long)\n",
    "\n",
    "# Usar 'proportion_agreement' como etiquetas\n",
    "edge_label = torch.tensor(aristas['proportion_agreement'].values, dtype=torch.float)\n",
    "\n",
    "# Paso 4: Dividir los datos manualmente usando train_test_split\n",
    "train_edges, temp_edges = train_test_split(aristas, test_size=0.30, random_state=42)\n",
    "val_edges, test_edges = train_test_split(temp_edges, test_size=0.50, random_state=42)\n",
    "\n",
    "# Extraer los edge_index y las etiquetas (valores continuos)\n",
    "train_edge_index = torch.tensor([train_edges['source'].values, train_edges['target'].values], dtype=torch.long)\n",
    "val_edge_index = torch.tensor([val_edges['source'].values, val_edges['target'].values], dtype=torch.long)\n",
    "test_edge_index = torch.tensor([test_edges['source'].values, test_edges['target'].values], dtype=torch.long)\n",
    "\n",
    "train_edge_label = torch.tensor(train_edges['proportion_agreement'].values, dtype=torch.float)\n",
    "val_edge_label = torch.tensor(val_edges['proportion_agreement'].values, dtype=torch.float)\n",
    "test_edge_label = torch.tensor(test_edges['proportion_agreement'].values, dtype=torch.float)\n",
    "\n",
    "# Crear los objetos Data para cada conjunto\n",
    "train_data = Data(x=node_features, edge_index=train_edge_index, edge_label=train_edge_label)\n",
    "val_data = Data(x=node_features, edge_index=val_edge_index, edge_label=val_edge_label)\n",
    "test_data = Data(x=node_features, edge_index=test_edge_index, edge_label=test_edge_label)\n",
    "\n",
    "# Paso 5: Definir el modelo GNN\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(-1, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        row, col = edge_index\n",
    "        z = torch.cat([z[row], z[col]], dim=-1)\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.encoder(x, edge_index)\n",
    "        return self.decoder(z, edge_index)\n",
    "\n",
    "# Paso 6: Entrenamiento del modelo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Model(hidden_channels=32).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_data.x, train_data.edge_index)  # Se usa edge_index en lugar de edge_label_index\n",
    "    target = train_data.edge_label\n",
    "    loss = F.mse_loss(pred, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    data = data.to(device)\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index)  # Se usa edge_index en lugar de edge_label_index\n",
    "    target = data.edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return float(rmse)\n",
    "@torch.no_grad()\n",
    "def predict(data):\n",
    "    data = data.to(device)\n",
    "    model.eval()\n",
    "    pred = model(data.x, data.edge_index)  # Predicciones del modelo\n",
    "    target = data.edge_label.float()  # Valores reales\n",
    "    return pred.cpu().numpy(), target.cpu().numpy()  # Devuelve como numpy arrays para facilitar su uso en Pandas\n",
    "\n",
    "# Entrenar el modelo por 300 épocas\n",
    "for epoch in range(1, 301):\n",
    "    train_data = train_data.to(device)\n",
    "    loss = train()\n",
    "    train_rmse = test(train_data)\n",
    "    val_rmse = test(val_data)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, Val: {val_rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Obtener los embeddings de los nodos después del entrenamiento\n",
    "@torch.no_grad()\n",
    "def obtener_embeddings(data):\n",
    "    data = data.to(device)\n",
    "    model.eval()\n",
    "    embeddings = model.encoder(data.x, data.edge_index)\n",
    "    return embeddings.cpu().numpy()  # Convertir a numpy array para facilitar cálculos de similitud\n",
    "\n",
    "# Obtener los embeddings de los nodos de test\n",
    "embeddings = obtener_embeddings(test_data)\n",
    "similitud = cosine_similarity(embeddings)\n",
    "\n",
    "# Crear un mapeo inverso de índices a nombres de parlamentarios\n",
    "index_to_parliamentarian = {idx: name for name, idx in parliamentarian_to_index.items()}\n",
    "\n",
    "# Crear un mapeo de nombres a índices de parlamentarios (para facilitar la búsqueda)\n",
    "parliamentarian_to_index = {name: idx for idx, name in index_to_parliamentarian.items()}\n",
    "\n",
    "# Función para obtener los nodos más similares por nombre\n",
    "def nodos_mas_parecidos_con_nombres(nodo_idx, top_k=5):\n",
    "    similitudes_nodo = similitud[nodo_idx]\n",
    "    nodos_parecidos = similitudes_nodo.argsort()[::-1][1:top_k+1]  # Obtener los índices de los nodos más parecidos\n",
    "    nombres_parecidos = [index_to_parliamentarian[nodo] for nodo in nodos_parecidos]  # Convertir índices a nombres\n",
    "    return nombres_parecidos, similitudes_nodo[nodos_parecidos]\n",
    "\n",
    "# Función para verificar si hay una arista entre dos nodos y obtener proportion_agreement y sector político del nodo encontrado\n",
    "def obtener_arista_df(df_aristas, nombre_1, nombre_2):\n",
    "    relacion = df_aristas[((df_aristas['parliamentarian_1'] == nombre_1) & (df_aristas['parliamentarian_2'] == nombre_2)) |\n",
    "                          ((df_aristas['parliamentarian_1'] == nombre_2) & (df_aristas['parliamentarian_2'] == nombre_1))]\n",
    "    if not relacion.empty:\n",
    "        # Si existe la relación, devolver True, proportion_agreement, y el sector del nodo encontrado (nombre_2)\n",
    "        proportion_agreement = relacion.iloc[0]['proportion_agreement']\n",
    "        sector_nodo_encontrado = relacion.iloc[0]['sector_2'] if relacion.iloc[0]['parliamentarian_2'] == nombre_2 else relacion.iloc[0]['sector_1']\n",
    "        return True, proportion_agreement, sector_nodo_encontrado\n",
    "    else:\n",
    "        # Si no existe relación, devolver False y None\n",
    "        return False, None, None\n",
    "\n",
    "# Función para manejar la consulta desde la interfaz gráfica\n",
    "def consultar_nodo():\n",
    "    nombre_nodo_consulta = combo.get()\n",
    "    if nombre_nodo_consulta in parliamentarian_to_index:\n",
    "        nodo_consulta_idx = parliamentarian_to_index[nombre_nodo_consulta]  # Obtener el índice del nodo consultado\n",
    "        \n",
    "        # Obtener el sector político del nodo consultado\n",
    "        sector_nodo_consulta = aristas.loc[aristas['parliamentarian_1'] == nombre_nodo_consulta, 'sector_1'].values[0]\n",
    "        resultado_label.config(text=f\"Sector político del nodo consultado: {sector_nodo_consulta}\")\n",
    "        \n",
    "        # Buscar los nodos más similares por nombre\n",
    "        nodos_parecidos_nombres, similitudes = nodos_mas_parecidos_con_nombres(nodo_consulta_idx, top_k=5)\n",
    "        \n",
    "        # Limpiar resultados previos\n",
    "        resultados_box.delete(1.0, tk.END)\n",
    "\n",
    "        # Imprimir resultados en la caja de texto\n",
    "        resultados_box.insert(tk.END, f\"Nombres de los nodos más parecidos:\")\n",
    "        for i, nodo in enumerate(nodos_parecidos_nombres):\n",
    "            resultados_box.insert(tk.END, f\"{nodo} (Similitud: {similitudes[i]:.4f})\\n\")\n",
    "\n",
    "            # Verificar aristas entre el nodo de consulta y los nodos más parecidos en el dataframe aristas\n",
    "            existe_arista, proportion_agreement, sector_nodo_encontrado = obtener_arista_df(aristas, nombre_nodo_consulta, nodo)\n",
    "            if existe_arista:\n",
    "                resultados_box.insert(tk.END, f\"Arista existente - Proportion Agreement: {proportion_agreement}, Sector: {sector_nodo_encontrado}\\n\\n\")\n",
    "            else:\n",
    "                # Obtener el sector político del nodo no encontrado en el dataframe 'nodos'\n",
    "                sector_nodo_no_encontrado = nodos.loc[nodos['parliamentarian_1'] == nodo, 'sector_1'].values[0]\n",
    "                resultados_box.insert(tk.END, f\"No hay arista con este nodo (Sector: {sector_nodo_no_encontrado})\\n\\n\")\n",
    "    else:\n",
    "        resultado_label.config(text=f\"El nodo con nombre {nombre_nodo_consulta} no existe.\")\n",
    "\n",
    "# Lista de nombres de parlamentarios (simulada, debe venir de tu dataframe original)\n",
    "nombres_nodos = list(parliamentarian_to_index.keys())\n",
    "\n",
    "# Crear la ventana principal\n",
    "root = tk.Tk()\n",
    "root.title(\"Consulta de Parlamentarios\")\n",
    "root.geometry(\"500x600\")  # Tamaño de la ventana\n",
    "\n",
    "# Etiqueta principal\n",
    "label = tk.Label(root, text=\"Selecciona un parlamentario para consultar\", font=(\"Arial\", 14))\n",
    "label.pack(pady=10)\n",
    "\n",
    "# Crear un menú desplegable (combobox) para seleccionar parlamentarios\n",
    "combo = ttk.Combobox(root, values=nombres_nodos, font=(\"Arial\", 12))\n",
    "combo.pack(pady=10)\n",
    "\n",
    "# Botón para consultar el nodo seleccionado\n",
    "boton = tk.Button(root, text=\"Consultar\", font=(\"Arial\", 12), command=consultar_nodo)\n",
    "boton.pack(pady=10)\n",
    "\n",
    "# Etiqueta donde se mostrará el nodo consultado\n",
    "resultado_label = tk.Label(root, text=\"\", font=(\"Arial\", 12))\n",
    "resultado_label.pack(pady=10)\n",
    "\n",
    "# Caja de texto donde se mostrarán los resultados\n",
    "resultados_box = tk.Text(root, height=20, width=60, font=(\"Arial\", 10))\n",
    "resultados_box.pack(pady=10)\n",
    "\n",
    "# Ejecutar la ventana\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
